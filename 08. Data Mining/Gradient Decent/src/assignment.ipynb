{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "#### This Assignment\n",
    "\n",
    "In this assignment, We will implement gradient descent, and show how it can be used to minimize differentiable functions, even including loss functions for non-linear models.\n",
    "\n",
    "Note that this assignment will use bold notation to represent vectors, i.e. $\\mathbf{x}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Distribution\n",
    "| Question | Points | \n",
    "|----------|--------|\n",
    "| 1 | 1 |\n",
    "| 2 | 2 |\n",
    "| 3a | 2 |\n",
    "| Total | 5 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8cbb07a2cc27f7d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Part 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 16\n",
    "np.set_printoptions(4)\n",
    "\n",
    "# We will use plot_3d helper function to help us visualize gradients\n",
    "from utils_0 import plot_3d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Load Data\n",
    "Load the data.csv file into a pandas dataframe.  \n",
    "Note that we are reading the data directly from the URL address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "\n",
    "part_1_data = pd.read_csv(\"https://github.com/DS-100/su20/raw/gh-pages/resources/assets/datasets/hw7_data.csv\", index_col=0)\n",
    "part_1_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## A Simple Model\n",
    "Let's start by examining our data and creating a simple model that can represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "First, run the cell below to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x, y, marker='.',c='orange')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    \n",
    "x = part_1_data['x']\n",
    "y = part_1_data['y']\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The data looks roughly linear, with some extra sinusoidal noise. For now, let's assume that the data follows some underlying linear model. We define the underlying linear model that predicts the value $y$ using the value $x$ as: $f_{\\theta^*}(x) = \\theta^* \\cdot x$\n",
    "\n",
    "Since we cannot find the value of the population parameter $\\theta^*$ exactly, we will assume that our dataset approximates our population and use our dataset to estimate $\\theta^*$. We denote an estimate with $\\theta$ and the fitted estimated chosen based on the data as $\\hat{\\theta}$. Our parameterized model is:\n",
    "\n",
    "$$\\Large\n",
    "f_{\\theta}(x) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Based on this equation, we will define the linear model function `linear_model` below to estimate $\\textbf{y}$ (the $y$-values) given $\\textbf{x}$ (the $x$-values) and $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    return theta * x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "the squared loss function is smooth and continuous. Let's use squared loss to evaluate our estimate $\\theta$, which we will use later to identify an optimal $\\theta$, denoted $\\hat{\\theta}$. Given observations $y$ and their corresponding predictions $\\hat{y}$, we can compute the average loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def average_squared_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the averge squared loss for observations y and predictions y_hat.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    return np.mean((y - y_hat) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Finally, we will visualize the average squared loss as a function of $\\theta$, where several different values of $\\theta$ are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- an array containing different estimates of the scalar theta\n",
    "    \"\"\" \n",
    "    avg_loss = np.array([average_squared_loss(linear_model(x, theta), y) for theta in thetas])\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(thetas, avg_loss)\n",
    "    plt.xlabel(\"Theta\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    \n",
    "thetas = np.linspace(-1, 5, 70)\n",
    "visualize(x, y, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that $\\hat{\\theta}$ is approximately 1.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Fitting our Simple Model\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "### Question 1\n",
    "Let's confirm our visual findings for the optimal $\\hat{\\theta}$.\n",
    "\n",
    "Recall from homework 5 that the analytical solution for the optimal $\\hat{\\theta}$ for the average squared loss is: \n",
    "\n",
    "$$\\hat{\\theta} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now that we have the analytic solution for $\\hat{\\theta}$, implement the function `find_theta` that calculates the numerical value of $\\hat{\\theta}$ based on our data $\\textbf{x}$, $\\textbf{y}$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    \n",
    "    #write your answer here\n",
    "    \n",
    "\n",
    "theta_hat_simple = find_theta(x, y)\n",
    "print(f'theta_hat = {theta_hat_simple}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, let's plot our risk function again using the `visualize` function. But this time, we will add a vertical line at the optimal value of theta (plot the line $\\theta = \\hat{\\theta}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = find_theta(x, y)\n",
    "visualize(x, y, thetas)\n",
    "plt.axvline(x=theta_opt, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We now have an optimal value for $\\theta$ that minimizes the empirical risk. We can use the scatter plot of the data and add the line $f_{\\hat{\\theta}}(x) = \\hat{\\theta} \\cdot \\textbf{x}$ using the $\\hat{\\theta}$ computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt_2 = find_theta(x, y)\n",
    "scatter(x, y)\n",
    "line_values = linear_model(x, theta_opt_2)\n",
    "plt.plot(x, line_values, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Great! It looks like our estimator $f_{\\hat{\\theta}}(x)$ is able to estimate the average $y$ for each $x$ quite well using a single parameter $\\theta$. \n",
    "\n",
    "The difference between the true $y$'s and the predictions is known as the residual, $\\textbf{r}=\\textbf{y}-\\hat{\\theta} \\cdot \\textbf{x}$. Below, we find the residual and plot the residuals corresponding to $x$ in a scatter plot. We also plot a horizontal line at $y=0$ to assist visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "    theta_hat = find_theta(x, y)\n",
    "    y_sin = y - linear_model(x, theta_hat)\n",
    "    plt.scatter(x, y_sin)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('residual (true y - estimated y)')\n",
    "    plt.title('Residual vs x for Linear Model')\n",
    "    plt.axhline(y=0, color='r')\n",
    "\n",
    "visualize_residual(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 2: Increasing Model Complexity\n",
    "\n",
    "It looks like the residual is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$ ( f_{\\boldsymbol{\\theta}}(x) = \\theta_1x + \\sin(\\theta_2x) ) $$\n",
    "\n",
    "\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, which we can represent in the vector, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Note that a general sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. Looking at the residual plot above, it looks like the residual is zero at x = 0, and the residual swings between -1 and 1. Thus, it seems reasonable to effectively set the scaling and phase shifting parameter ($a$ and $c$ in this case) to 1 and 0 respectively. While we could try to fit $a$ and $c$, we're unlikely to get much benefit. When you're done with this assignment, you can try adding $a$ and $c$ to our model and fitting these parameters to see if you can get a better loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We define the `sin_model` function below that predicts $\\textbf{y}$ (the $y$-values) using $\\textbf{x}$ (the $x$-values) based on our new equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- a vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2\n",
    "    \"\"\"\n",
    "    theta_1 = theta[0]\n",
    "    theta_2 = theta[1]\n",
    "    return theta_1 * x + np.sin(theta_2 * x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Question 2\n",
    "Now, implement the functions `sin_MSE`, `sin_MSE_dt1` and `sin_MSE_dt2`, which should compute $R$, $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ respectively. Use the expressions you wrote for $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$. We have completed `sin_MSE_gradient` for you.\n",
    "\n",
    "Notes: \n",
    "* Keep in mind that we are still working with our original set of data, `part_1_data`\n",
    "* To keep your code a bit more concise, be aware that `np.mean` does the same thing as `np.sum` divided by the length of the numpy array.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c-answer-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_MSE(theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the l2 loss of our sinusoidal model given theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    \n",
    "    #write your answer here\n",
    "    \n",
    "\n",
    "def sin_MSE_dt1(theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "\n",
    "    #write your answer here\n",
    "\n",
    "    \n",
    "def sin_MSE_dt2(theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    \n",
    "    #write your answer here\n",
    "\n",
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def sin_MSE_gradient(theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    x = part_1_data['x']\n",
    "    y = part_1_data['y']     \n",
    "    return np.array([sin_MSE_dt1(theta), sin_MSE_dt2(theta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(sin_MSE_dt2([0, np.pi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3: Gradient Descent\n",
    "\n",
    "\n",
    "Let's now implement gradient descent. \n",
    "\n",
    "Note that the function you're implementing here is somewhat different than the gradient descent function we created in lecture. The version in lecture was `gradient_descent(df, initial_guess, alpha, n)`, where `df` was the gradient of the function we are minimizing and `initial_guess` are the starting parameters for that function. Here our signature is a bit different (described below).\n",
    "\n",
    "### Question 3\n",
    "Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in an array for $\\textbf{x}$ (`x`), an array for $\\textbf{y}$ (`y`), and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate (i.e. the same learning rate at every time step), just like in lecture.\n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the average squared loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "After completing the function, the cell will output the trajectory from running gradient descent over time.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times.\n",
    "- Recall that the gradient descent update function follows the form:\n",
    "$$ \n",
    "\\boldsymbol{\\theta}^{(t+1)} \\leftarrow \\boldsymbol{\\theta}^{(t)} - \\alpha \\left(\\nabla_{\\boldsymbol{\\theta}} \\mathbf{R}(\\mathbf{x}, \\mathbf{y}, \\boldsymbol{\\theta}^{(t)}) \\right)\n",
    "$$\n",
    "- Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,))\n",
    "\n",
    "def grad_desc(loss_f, gradient_loss_f, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    #write your answer here\n",
    "   \n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat, thetas_used, losses_calculated = grad_desc(sin_MSE, sin_MSE_gradient, theta_start, num_iter=20, alpha=0.1)\n",
    "for b, l in zip(thetas_used, losses_calculated):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
